
# PROJECT: API - Machinery Data Collection Automation

This project consists of an API developed to centralize and automate the machinery data‑collection process from specific websites.

The goal is to ensure that the collection process occurs in an efficient, programmable, and long‑term sustainable manner, allowing automatic updates whenever necessary; thus, whenever it is necessary to update these same data, just access them here.

**Currently Supported Site**

- [BALDOR](https://www.baldor.com/): data collection from this site is already implemented and working through the API.

## DEVELOPMENT

All development was carried out in `Python`, using the following technologies:

For the API:

- `FastAPI`: chosen for its performance, simplicity, and native support for interactive documentation via Swagger.

For data collection:

- `Playwright`: used to automate navigation on pages with dynamic content, efficiently simulating human interaction.

- `Requests`: employed on pages whose content can be accessed directly via HTTP requests, speeding up the extraction process.

### FOLDER STRUCTURE

The files in the folders are distributed as follows:

```bash
project/
├── app/
│   ├── output/                          # Files that contain the data generated by the system
│   │   └── assets/
│   │
│   ├── scraping/                        # Scraping logic
│   │   ├── baldor
│   │   │   ├── baldor_scraping.py       # File with the Baldor site scraping functions
│   │   │   ├── baldor_output.py         # File responsible for processing and returning the requested output
│   │   │   └── final_output.py          # File responsible for processing and returning the final API output
│   │   │
│   │   └── other_sites                  # Recommended location for future scraped sites
│   │
│   ├── utils/
│   │   ├── base64_converter.py          # File responsible for encoding and decoding Base64 data
│   │   ├── download_files.py            # File responsible for downloading data to the specified folder
│   │   ├── general_utils.py             # File responsible for basic functions such as logs, checkpoints, among others
│   │   └── pre_process.py               # File responsible for preprocessing functions
│   │
│   └── main.py                          # FastAPI main execution file
│
├── Dockerfile
├── startup.sh
├── requirements.txt
├── .gitignore
└── README.md                            # This file
```

### RESPONSE

The API has a `POST` endpoint that executes the collection process per site, with the main difference being in the response format:

- **/query**: returns a JSON object containing the same data, but with the files embedded in Base64 format.

#### RESPONSE EXAMPLE

The response will be a `.json` file containing:

- **metadados.json**: structured product information.

Example JSON response:

```json
{
  "product_id": "M123456",
  "name": "Three‑phase AC Motor",
  "description": "TEFC, 2 HP, 1800 RPM",
  "specs": {
    "hp": "2",
    "voltage": "230/460",
    "rpm": "1800",
    "frame": "145T"
  },
  "bom": [
    {
      "part_number": "123-456",
      "description": "Cooling fan",
      "quantity": 1
    }
  ],
  "assets": {
    "manual": "assets/M123456/manual.pdf",
    "cad": "assets/M123456/cad.dwg",
    "image": "assets/M123456/img.jpg"
  },
  "docs_base64": {
    "manual": "SLJKDHFKHGA/.../SKA20935423H/KJB42352342/2342;5EÇRWÉ",
    "cad": "SLJKDHFKHGA/.../SKA20935423H/KJB42352342/2342;5EÇRWÉ",
    "image": "SLJKDHFKHGA/.../SKA20935423H/KJB42352342/2342;5EÇRWÉ"
  }
}
```

API functioning visualization with the response:

<video controls src="app/data/api_example.mp4" title="Title" style="width: 100%;"></video>

## RUNNING THE PROJECT

The project was developed and structured to scrape all products from the site. Because the site can block access due to excessive scraping, methods were added to address this, namely cookie cleaning and a wait point that pauses until the system returns, saving the process up to that point so it can resume from there.

It is worth noting that even though the project was designed to collect all machinery data from the site, a limitation was added to the process, causing it to stop and move to the next step after collecting 15 products. If you want this limitation to be higher or removed, go to "scraping/baldor_scraping.py" and delete or comment out the following snippet:

```python
########################################################################

if len(products) >= 15:
    logging.info("  |_ Limit of 15 products reached. Stopping collection.")
    subsubcat["product"] = products
    return data 

########################################################################
```

To run the project, simply install Docker and, via WSL in VS Code, open your terminal and execute the following command:
```bash
docker build -t scrap .
```
```bash
docker run -p 5000:5050 scrap
```

You can monitor performance and errors in the logs, as shown in the following example:

<video controls src="app/data/logs_example.mp4" title="Title" style="width: 100%;"></video>

